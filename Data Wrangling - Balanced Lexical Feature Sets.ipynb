{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project's approach will rely mainly on lexical features derived from individual url links. Therefore, afer import of collected benign, phishing and malicious url lists, new features will be created from the base url links. \n",
    "\n",
    "Besides lexical features, this project will leverage www.alexa.com to determine if a url's domain exists in Alexa's Top 500 website list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "#import relevant modules\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sources:\n",
    "\n",
    "Malicious URLs:\n",
    "A sample of 10,000 urls is taken from a csv record of over 600,000 malicious url links retrieved from https://urlhaus.abuse.ch. URLhaus is a project operated by abuse.ch. The project collects and shares malware URLs, to assist network administrators and security analysts in protecting their networks from cyber threats.\n",
    "\n",
    "Phishing URLs:\n",
    "A sample of 10,000 urls is taken from a csv record of over 17,000 phishing url links retrieved from http://phishtank.org/. PhishTank is a collaborative clearing house for data and information about phishing on the Web. It's url lists are available to developers to integrate anti-phishing data into their applications.  \n",
    "\n",
    "Benign URLs:\n",
    "Over 25,000 urls were collected by crawling Alexa's list of the top 2500 websites. In order to help validate that each url was 'benign', each url's reputation was checked via VirusTotal. VirusTotal inspects urls with over 70 antivirus scanners and URL/domain blacklisting services, as well as other tools. Virus scans were requested in those instances where a url had no previous scans or reporting available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv file of phishing urls\n",
    "ph_df = pd.read_csv('phishing_urls.csv')     \n",
    "\n",
    "# Reduce dataset to reflect only urls verified and online\n",
    "ph_df = ph_df[(ph_df['verified'] == 'yes') & (ph_df['online'] == 'yes')]\n",
    "\n",
    "# drop unnecessary features\n",
    "drop = ['phish_id', 'target', 'phish_detail_url', 'submission_time', 'verification_time', 'verified', 'online']\n",
    "ph_df = ph_df.drop(drop, axis=1)\n",
    "\n",
    "# assign category value 'phishing'\n",
    "ph_df['category'] = 'phishing'\n",
    "\n",
    "# take a sample of 10,000 records\n",
    "ph_df_sample = ph_df.sample(n=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in benign urls, drop unnecessary features\n",
    "b_df = pd.read_csv('alexa_urls.csv')  \n",
    "drop = ['scheme', 'netloc', 'path', 'params', 'query', 'fragment']\n",
    "b_df = b_df.drop(drop, axis=1)\n",
    "\n",
    "# assign category value 'benign'\n",
    "b_df['category'] = 'benign'\n",
    "\n",
    "b_df_sample = b_df.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in malicious urls\n",
    "m_df = pd.read_csv('malicious_urls2.csv')\n",
    "\n",
    "# assign category value 'malicious'\n",
    "m_df['category'] = 'malicious'\n",
    "\n",
    "# take a sample of 10,000 records\n",
    "m_df_sample = m_df.sample(n=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat all dataframes\n",
    "df = pd.concat([b_df_sample, ph_df_sample, m_df_sample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy\n",
    "df.to_pickle('capstone2_data_balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build out new features based on lexical analysis of the url and its components: scheme, netloc, path, params, query and fragment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('capstone2_data'_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the urls into components: scheme, netloc, path, params, query and fragment.\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "df['scheme'],df['netloc'],df['path'],df['params'],df['query'],df['fragment'] = zip(*df['url'].map(urlparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build url length features\n",
    "df['len_url'] = df['url'].apply(len) # url length\n",
    "df['is_53'] = (df['len_url'] < 54) # is url less than 54 char\n",
    "df['is_54_75'] = (df['len_url'] > 53) & (df['len_url'] < 76) # is url between 54 and 75 char\n",
    "df['is_76'] = (df['len_url'] > 75) # is url greater than 75 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split url into tokens and create new features: list of url's tokens, number of url tokens, average token length \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenized_url = []\n",
    "len_tokenized = []\n",
    "\n",
    "for url in df.url:\n",
    "    result = WordPunctTokenizer().tokenize(url)\n",
    "    tokenized_url.append(result)\n",
    "    len_tokenized.append(len(result))\n",
    "    \n",
    "df['len_tokenized_url'] = len_tokenized\n",
    "df['avg_token_len'] = df['len_url']/df['len_tokenized_url'] \n",
    "df['tokenized_url'] = tokenized_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new numeric feature identifying location of last set of '//' in url\n",
    "df['last_slashes'] = df.apply(lambda row: row.url.rfind('//'), axis=1)\n",
    "\n",
    "# create feature speciyfing % location of slashes within the url\n",
    "df['loc_last_slashes'] = df['last_slashes']/df['len_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features reflecting character content\n",
    "\n",
    "def pc_upper_lower(string):\n",
    "    # Based on the a-zA-Z characters in a string, \n",
    "    # calculates the percentage of uppercase and lowercase characters.\n",
    "    \n",
    "    alpha = uppercase = lowercase = n_letters = 0 \n",
    "    if len(string) >= 1:\n",
    "        for char in string:\n",
    "            if char.isalpha():\n",
    "                alpha += 1\n",
    "                if char.isupper():\n",
    "                    uppercase += 1\n",
    "                if char.islower():\n",
    "                    lowercase += 1\n",
    "        n_letters = uppercase + lowercase\n",
    "        if n_letters != 0:\n",
    "            percent_upper = round(uppercase/n_letters, 3)\n",
    "            percent_lower = round(lowercase/n_letters, 3)\n",
    "        else:\n",
    "            percent_upper = 0\n",
    "            percent_lower = 0\n",
    "    else:\n",
    "        percent_upper = 0\n",
    "        percent_lower = 0 \n",
    "    return percent_upper, percent_lower\n",
    "\n",
    "\n",
    "url_list = df['url'].tolist()\n",
    "\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "\n",
    "percent_alpha = []\n",
    "percent_num = []\n",
    "percent_special = []\n",
    "\n",
    "num_dots = []\n",
    "num_at_signs = []\n",
    "num_semicolons = []\n",
    "num_underscores = []\n",
    "num_question_marks = []\n",
    "upper_percent = []\n",
    "lower_percent = []\n",
    "\n",
    "for i in url_list:\n",
    "    numbers = sum(c.isdigit() for c in i)\n",
    "    num.append(numbers)\n",
    "    percent_num.append(numbers/len(i))\n",
    "    \n",
    "    letters = sum(c.isalpha() for c in i)\n",
    "    let.append(letters)\n",
    "    percent_alpha.append(letters/len(i))\n",
    "    \n",
    "    others = len(i) - numbers - letters\n",
    "    spec.append(others)\n",
    "    percent_special.append(others/len(i))\n",
    "    \n",
    "    num_dots.append(i.count('.'))\n",
    "    num_at_signs.append(i.count('@'))\n",
    "    num_semicolons.append(i.count(';'))\n",
    "    num_underscores.append(i.count('_'))\n",
    "    num_question_marks.append(i.count('?'))\n",
    "\n",
    "    percent_upper, percent_lower = pc_upper_lower(i)\n",
    "    upper_percent.append(percent_upper)\n",
    "    lower_percent.append(percent_lower)\n",
    "\n",
    "df['n_let'] = let \n",
    "df['n_num'] = num\n",
    "df['n_spec'] = spec\n",
    "df['pc_num'] = percent_num\n",
    "df['pc_let'] = percent_alpha\n",
    "df['pc_spec'] = percent_special\n",
    "df['n_dots'] = num_dots\n",
    "df['n_ats'] = num_at_signs\n",
    "df['n_semicol'] = num_semicolons\n",
    "df['num_underscores'] = num_underscores\n",
    "df['num_question'] = num_question_marks\n",
    "df['pc_uppercase'] = upper_percent\n",
    "df['pc_lowercase'] = lower_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate an Shannon entropy score for each url\n",
    "# urls with a larger characters distribution will have a higher score\n",
    "\n",
    "import math\n",
    "\n",
    "def shannon(word):\n",
    "    entropy = 0.0\n",
    "    length = len(word)\n",
    "    occ = {}\n",
    "    for c in word :\n",
    "        if not c in occ:\n",
    "            occ[ c ] = 0\n",
    "        else:\n",
    "            occ[ c ] = occ[c] + 1\n",
    "\n",
    "    for (k,v) in occ.items(): # changed from iteritems\n",
    "        p = float( v ) / float(length)\n",
    "        if p > 0: # added this to avoid math domain error where p = 0\n",
    "            entropy -= p * math.log(p, 2) # Log base 2\n",
    "    return entropy\n",
    "\n",
    "url_entropy_result = []\n",
    "\n",
    "for i in url_list:\n",
    "    url_entropy_result.append(shannon(i))\n",
    "    \n",
    "df['entropy'] = url_entropy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a masque feature: the number of letter+digit+letter substrings in a url\n",
    "# create a character continuity rate feature: (length of longest letter substring + length of longest digit substring +\n",
    "# length of longest special character substring) / URL length\n",
    "\n",
    "import re\n",
    "\n",
    "def masque_count(token):\n",
    "    match = re.findall(\"([a-zA-Z][0-9][a-zA-Z])\", token)\n",
    "    match_count = len(match)\n",
    "    return match_count\n",
    "\n",
    "def longestSubstring(str):\n",
    "    \n",
    "    # find the longest consecutive substring of a certain type\n",
    "    \n",
    "    l = re.findall(r'[A-Za-z]+', str)\n",
    "    d = re.findall(r'\\d+', str)\n",
    "    s = re.findall(r'[^a-zA-Z0-9]+', str)\n",
    "    if l:  #THESE WONT CALC IF EMPTY LIST\n",
    "        ll = max(l, key = len)\n",
    "        max_l = len(ll)\n",
    "    else:\n",
    "        max_l = 0\n",
    "    if d: \n",
    "        ld = max(d, key = len)\n",
    "        max_d = len(ld)\n",
    "    else:\n",
    "        max_d = 0\n",
    "    if s:\n",
    "        ls = max(s, key = len)\n",
    "        max_s = len(ls)\n",
    "    else:\n",
    "        max_s = 0\n",
    "        \n",
    "    char_total = (max_l + max_d + max_s)\n",
    "    char_cont_rate = char_total/len(str)\n",
    "    return char_cont_rate\n",
    "\n",
    "\n",
    "url_masques = [] \n",
    "cont_rate = []\n",
    "                    \n",
    "for url in url_list:\n",
    "    url_masques.append(masque_count(url))\n",
    "    cont_rate.append(longestSubstring(url))\n",
    "\n",
    "df['n_masques'] = url_masques\n",
    "df['char_cont_rate'] = cont_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reg_domain and domain_suffix features, by extracting second-level and top-level domain section. e.g. example.com\n",
    "# identify ip addresses used in place of domain\n",
    "\n",
    "# import tldextract to parse out true registered domain\n",
    "import tldextract \n",
    "\n",
    "# adding a function that identifies an ip address\n",
    "def is_ipv4(ip):\n",
    "    match = re.match(\"^(\\d{0,3})\\.(\\d{0,3})\\.(\\d{0,3})\\.(\\d{0,3})\", ip)\n",
    "    if not match:\n",
    "        return False\n",
    "    quad = []\n",
    "    for number in match.groups():\n",
    "        quad.append(int(number))\n",
    "    if quad[0] < 1:\n",
    "        return False\n",
    "    for number in quad:\n",
    "        if number > 255 or number < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "reg_domain = []\n",
    "domain_suffix = []\n",
    "\n",
    "for i in url_list:\n",
    "    ext = tldextract.extract(i)\n",
    "    reg = '.'.join(ext[1:]) #this returns domain + suffixes or ip + .\n",
    "    sub_domain = '.'.join(ext[:2])\n",
    "    suffix = ext.suffix\n",
    "    domain_suffix.append(suffix)\n",
    "    if is_ipv4(reg): # if reg is an ip, drop the . at the end of ip\n",
    "        reg = str(reg)[:-1]\n",
    "    \n",
    "    reg_domain.append(reg) #append list with the domain+suffix, or ip address\n",
    "    \n",
    "#add domain as new df feature \n",
    "df['reg_domain'] = reg_domain\n",
    "df['domain_suffix'] = domain_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature identifying the number of domain suffixes (top-level domains)\n",
    "\n",
    "num_domain_suffix = []\n",
    "\n",
    "for i in df.domain_suffix:\n",
    "    if i:\n",
    "        num_domain_suffix.append(i.count('.') + 1)\n",
    "    else: \n",
    "        num_domain_suffix.append(0)\n",
    "\n",
    "\n",
    "df['n_domain_suffix'] = num_domain_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use prior is_ip function to create new boolean feature. \n",
    "# create new features based on reg_domain characters\n",
    "\n",
    "IP = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = []\n",
    "domain_dots = []\n",
    "domain_len = []\n",
    "domain_hyphens = []\n",
    "domain_ats = []\n",
    "domain_masques = []\n",
    "domain_entropy_result = []\n",
    "\n",
    "for i in reg_domain:\n",
    "    response = is_ipv4(i)\n",
    "    IP.append(response)\n",
    "    \n",
    "    num.append(numbers)\n",
    "    percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "    letters = sum(c.isalpha() for c in i)\n",
    "    let.append(letters)\n",
    "    percent_chars.append(letters/len(i))\n",
    "    \n",
    "    others = len(i) - numbers - letters\n",
    "    spec.append(others)\n",
    "    percent_others.append(others/len(i))\n",
    "    \n",
    "    domain_dots.append(i.count('.'))\n",
    "    domain_len.append(len(i))\n",
    "    domain_hyphens.append(i.count('-'))\n",
    "    domain_ats.append(i.count('@'))                  \n",
    "                    \n",
    "    domain_masques.append(masque_count(i))\n",
    "    domain_entropy_result.append(shannon(i))\n",
    "\n",
    "df['len_domain'] = domain_len\n",
    "df['is_ip'] = IP #add boolean list to df\n",
    "df['n_domain_num'] = num\n",
    "df['n_domain_let'] = let\n",
    "df['n_domain_spec'] = spec\n",
    "df['pc_domain_num'] = percent_numbers\n",
    "df['pc_domain_let'] = percent_chars\n",
    "df['pc_domain_spec'] = percent_others\n",
    "df['n_domain_dots'] = domain_dots\n",
    "df['n_domain_tok'] = df['n_domain_dots'] + 1\n",
    "df['avg_domain_tok_len'] = df['len_domain']/df['n_domain_tok']\n",
    "df['n_domain_hyphens'] = domain_hyphens\n",
    "df['n_domain_ats'] = domain_ats\n",
    "df['n_domain_masques'] = domain_masques\n",
    "df['domain_entropy'] = domain_entropy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create is_top_500_domain (a boolean feature) to report if reg_domain is in Alexa's top 500 domain list\n",
    "\n",
    "# read in top 500 websites\n",
    "top_500_domains = pd.read_csv('alexa_top500.csv')\n",
    "\n",
    "top_500_list = top_500_domains.domain.to_list()\n",
    "\n",
    "#adding a function that identifies whether a domain is in the top 500 domain list\n",
    "def is_match(domain, my_list):\n",
    "    if domain in my_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_top_500_domain = []\n",
    "\n",
    "for domain in reg_domain:\n",
    "    if is_ipv4(domain):\n",
    "        is_top_500_domain.append(False)\n",
    "    else:\n",
    "        is_top_500_domain.append(is_match(domain, top_500_list))\n",
    "    \n",
    "df['is_top500_domain'] = is_top_500_domain # likely FALSE for suspicious/malicious urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features based on url's netloc section (netloc = subdomain + domain + suffix) in comparison to exisitng reg_domain feature\n",
    "\n",
    "num_netloc_dots = []\n",
    "len_netloc = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "netloc_masques = []\n",
    "netloc_entropy_result = []\n",
    "n_subs = []\n",
    "\n",
    "netloc = df.netloc\n",
    "\n",
    "for i in netloc:\n",
    "    dots = i.count('.')\n",
    "    num_netloc_dots.append(dots)\n",
    "    len_netloc.append(len(i))\n",
    "\n",
    "    numbers = sum(c.isdigit() for c in i)\n",
    "    num.append(numbers)\n",
    "    percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "    letters = sum(c.isalpha() for c in i)\n",
    "    let.append(letters)\n",
    "    percent_chars.append(letters/len(i))\n",
    "    \n",
    "    others = len(i) - numbers - letters\n",
    "    spec.append(others)\n",
    "    percent_others.append(others/len(i))\n",
    "    \n",
    "    netloc_masques.append(masque_count(i))\n",
    "    \n",
    "    netloc_entropy_result.append(shannon(i))\n",
    "    \n",
    "\n",
    "df['n_netloc_dots'] = num_netloc_dots\n",
    "df['len_netloc'] = len_netloc\n",
    "df['n_netloc_num'] = num\n",
    "df['n_netloc_let'] = let\n",
    "df['n_netloc_spec'] = spec\n",
    "df['pc_netloc_num'] = percent_numbers\n",
    "df['pc_netloc_let'] = percent_chars\n",
    "df['pc_netloc_spec'] = percent_others\n",
    "df['n_netloc_tok'] = df['n_netloc_dots'] + 1\n",
    "df['n_subdomains'] = (df['n_netloc_tok'] - df['n_domain_tok'])\n",
    "df['avg_netloc_tok_len'] = df['len_netloc']/(df['n_netloc_tok'])\n",
    "df['n_netloc_masques'] = netloc_masques\n",
    "df['netloc_entropy'] = netloc_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature with list of all paths within url\n",
    "\n",
    "path_items = []\n",
    "\n",
    "for i in df.path:\n",
    "    path_list = (re.split('/', i))\n",
    "    path_list = [x for x in path_list if x != \"\"]\n",
    "    path_items.append(path_list)\n",
    "    \n",
    "df['path_items'] = path_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional path features based on characters\n",
    "\n",
    "path_slashes = []\n",
    "path_20 = []\n",
    "len_total_path = []\n",
    "percent_numbers = []\n",
    "percent_letters = []\n",
    "percent_others = [] \n",
    "path_masques = []\n",
    "path_entropy_result = []\n",
    "\n",
    "for i in df.path:\n",
    "    path_slashes.append(i.count('/'))\n",
    "    path_20.append(i.count('/%20'))\n",
    "    len_total_path.append(len(i))\n",
    "    \n",
    "    if len(i) > 0:\n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        percent_letters.append(letters/len(i))\n",
    "        others = len(i) - numbers - letters\n",
    "        percent_others.append(others/len(i))\n",
    "        path_masques.append(masque_count(i))\n",
    "        path_entropy_result.append(shannon(i))\n",
    "    \n",
    "    else:\n",
    "        percent_numbers.append(0)\n",
    "        percent_letters.append(0)\n",
    "        percent_others.append(0)\n",
    "        path_masques.append(0)\n",
    "        path_entropy_result.append(0)\n",
    "\n",
    "df['len_all_paths'] = len_total_path    \n",
    "df['n_path_slashes'] = path_slashes\n",
    "df['n_path_pc20'] = path_20\n",
    "df['pc_path_num'] = percent_numbers\n",
    "df['pc_path_let'] = percent_letters\n",
    "df['pc_path_spec'] = percent_others\n",
    "df['n_path_masques'] = path_masques\n",
    "df['path_entropy'] = path_entropy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path features based on length of individual path items\n",
    "\n",
    "def path_lengths(list_of_paths):\n",
    "    max = 0\n",
    "    min = 500\n",
    "    single = 0\n",
    "    for i in list_of_paths:\n",
    "        if len(i) < min:\n",
    "             min = len(i)    \n",
    "        if len(i) > max:\n",
    "            max = len(i)    \n",
    "        if len(i) == 1: \n",
    "            single += 1        \n",
    "    num_items = len(list_of_paths)    \n",
    "    return min, max, single, num_items\n",
    "\n",
    "path_shortest_item_len = []\n",
    "path_longest_item_len = []\n",
    "num_single_char_path = []\n",
    "num_path_items = []\n",
    "\n",
    "for paths in df.path_items:\n",
    "    if len(paths) > 0:\n",
    "        min, max, single, num_items = path_lengths(paths)\n",
    "        path_shortest_item_len.append(min)\n",
    "        path_longest_item_len.append(max)\n",
    "        num_single_char_path.append(single)\n",
    "        num_path_items.append(num_items)\n",
    "    else:\n",
    "        path_shortest_item_len.append(0)\n",
    "        path_longest_item_len.append(0)\n",
    "        num_single_char_path.append(0)\n",
    "        num_path_items.append(0)\n",
    "    \n",
    "df['shortest_path_len'] = path_shortest_item_len\n",
    "df['longest_path_len'] = path_longest_item_len\n",
    "df['n_single_char_path'] = num_single_char_path\n",
    "df['n_path_items'] = num_path_items\n",
    "df['avg_path_token_len'] = (df['len_all_paths'])/(df['n_path_items'])\n",
    "\n",
    "# switch out any 'inf' values with NAN\n",
    "df.loc[~np.isfinite(df['avg_path_token_len']), 'avg_path_token_len'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Path section, calculate percent of upper and lowercase letters (of total letters)\n",
    "\n",
    "upper_percent = []\n",
    "lower_percent = []\n",
    "\n",
    "paths = df.path\n",
    "\n",
    "for i in paths:\n",
    "    percent_upper, percent_lower = pc_upper_lower(i)\n",
    "    upper_percent.append(percent_upper)\n",
    "    lower_percent.append(percent_lower)\n",
    "    \n",
    "df['pc_path_uppercase'] = upper_percent\n",
    "df['pc_path_lowercase'] = lower_percent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features based on parameter section of url\n",
    "\n",
    "len_parameters = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "params_masques = []\n",
    "param_entropy_result = []\n",
    "\n",
    "for i in df.params:\n",
    "    length = len(i)\n",
    "    len_parameters.append(length)\n",
    "    \n",
    "    if i:\n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        num.append(numbers)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        let.append(letters)\n",
    "        percent_chars.append(letters/len(i))\n",
    "    \n",
    "        others = len(i) - numbers - letters\n",
    "        spec.append(others)\n",
    "        percent_others.append(others/len(i))\n",
    "    \n",
    "        params_masques.append(masque_count(i))\n",
    "    \n",
    "        param_entropy_result.append(shannon(i))\n",
    "    \n",
    "    else:\n",
    "        num.append(0)\n",
    "        let.append(0)\n",
    "        spec.append(0)\n",
    "        percent_numbers.append(0)\n",
    "        percent_chars.append(0)\n",
    "        percent_others.append(0)\n",
    "        params_masques.append(0)\n",
    "        param_entropy_result.append(0)\n",
    "        \n",
    "df['len_param'] = len_parameters\n",
    "df['n_param_num'] = num\n",
    "df['n_param_let'] = let\n",
    "df['n_param_spec'] = spec\n",
    "df['pc_param_num'] = percent_numbers\n",
    "df['pc_param_let'] = percent_chars\n",
    "df['pc_param_spec'] = percent_others\n",
    "df['n_params_masque'] = params_masques\n",
    "df['param_entropy'] = param_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features based on query section of url\n",
    "\n",
    "queries = df['query']\n",
    "num_queries = []\n",
    "len_query = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "queries_masques = []\n",
    "queries_entropy_result = []\n",
    "\n",
    "\n",
    "for i in queries:\n",
    "    if len(i) > 0:\n",
    "        num_queries.append(i.count(';') + 1) \n",
    "        len_query.append(len(i))\n",
    "        \n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        num.append(numbers)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "        \n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        let.append(letters)\n",
    "        percent_chars.append(letters/len(i))\n",
    "        \n",
    "        others = len(i) - numbers - letters\n",
    "        spec.append(others)\n",
    "        percent_others.append(others/len(i))\n",
    "    \n",
    "        queries_masques.append(masque_count(i))\n",
    "        \n",
    "        queries_entropy_result.append(shannon(i))\n",
    "    else:\n",
    "        num_queries.append(0)\n",
    "        len_query.append(0)\n",
    "        num.append(0)\n",
    "        let.append(0)\n",
    "        spec.append(0)\n",
    "        percent_numbers.append(0)\n",
    "        percent_chars.append(0)\n",
    "    \n",
    "        percent_others.append(0)\n",
    "    \n",
    "        queries_masques.append(0)\n",
    "    \n",
    "        queries_entropy_result.append(0)\n",
    "        \n",
    "df['n_queries'] = num_queries\n",
    "df['len_query'] = len_query\n",
    "df['n_query_num'] = num\n",
    "df['n_query_let'] = let\n",
    "df['n_query_spec'] = spec\n",
    "df['pc_query_num'] = percent_numbers\n",
    "df['pc_query_let'] = percent_chars\n",
    "df['pc_query_spec'] = percent_others\n",
    "df['n_queries_masques'] = queries_masques\n",
    "df['queries_entropy'] = queries_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fragment Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features based on fragment section\n",
    "\n",
    "len_fragment = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "frag_masques = []\n",
    "frag_entropy_result = []\n",
    "\n",
    "for i in df.fragment:\n",
    "    len_frag = len(i)\n",
    "    len_fragment.append(len_frag)\n",
    "    \n",
    "    if len_frag > 0:\n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        num.append(numbers)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        let.append(letters)\n",
    "        percent_chars.append(letters/len(i))\n",
    "    \n",
    "        others = len(i) - numbers - letters\n",
    "        spec.append(others)\n",
    "        percent_others.append(others/len(i))\n",
    "    \n",
    "        frag_masques.append(masque_count(i))\n",
    "    \n",
    "        frag_entropy_result.append(shannon(i))\n",
    "    \n",
    "    else:\n",
    "        num.append(0)\n",
    "        let.append(0)\n",
    "        spec.append(0)\n",
    "        percent_numbers.append(0)\n",
    "        percent_chars.append(0)\n",
    "        percent_others.append(0)\n",
    "        frag_masques.append(0)\n",
    "        frag_entropy_result.append(0)\n",
    "        \n",
    "df['len_frag'] = len_fragment\n",
    "df['n_frag_num'] = num\n",
    "df['n_frag_let'] = let\n",
    "df['n_fraf_spec'] = spec\n",
    "df['pc_frag_num'] = percent_numbers\n",
    "df['pc_frag_let'] = percent_chars\n",
    "df['pc_frag_spec'] = percent_others\n",
    "df['n_frag_masques'] = frag_masques\n",
    "df['frag_entropy'] = frag_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of dataframe\n",
    "df.to_pickle('capstone2_balanced_withfeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('capstone2_balanced_withfeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>scheme</th>\n",
       "      <th>netloc</th>\n",
       "      <th>path</th>\n",
       "      <th>params</th>\n",
       "      <th>query</th>\n",
       "      <th>fragment</th>\n",
       "      <th>len_url</th>\n",
       "      <th>is_53</th>\n",
       "      <th>is_54_75</th>\n",
       "      <th>is_76</th>\n",
       "      <th>len_tokenized_url</th>\n",
       "      <th>avg_token_len</th>\n",
       "      <th>tokenized_url</th>\n",
       "      <th>last_slashes</th>\n",
       "      <th>loc_last_slashes</th>\n",
       "      <th>n_let</th>\n",
       "      <th>n_num</th>\n",
       "      <th>n_spec</th>\n",
       "      <th>pc_num</th>\n",
       "      <th>pc_let</th>\n",
       "      <th>pc_spec</th>\n",
       "      <th>n_dots</th>\n",
       "      <th>n_ats</th>\n",
       "      <th>n_semicol</th>\n",
       "      <th>num_underscores</th>\n",
       "      <th>num_question</th>\n",
       "      <th>pc_uppercase</th>\n",
       "      <th>pc_lowercase</th>\n",
       "      <th>entropy</th>\n",
       "      <th>n_masques</th>\n",
       "      <th>char_cont_rate</th>\n",
       "      <th>reg_domain</th>\n",
       "      <th>domain_suffix</th>\n",
       "      <th>n_domain_suffix</th>\n",
       "      <th>len_domain</th>\n",
       "      <th>is_ip</th>\n",
       "      <th>n_domain_num</th>\n",
       "      <th>n_domain_let</th>\n",
       "      <th>n_domain_spec</th>\n",
       "      <th>pc_domain_num</th>\n",
       "      <th>pc_domain_let</th>\n",
       "      <th>pc_domain_spec</th>\n",
       "      <th>n_domain_dots</th>\n",
       "      <th>n_domain_tok</th>\n",
       "      <th>avg_domain_tok_len</th>\n",
       "      <th>n_domain_hyphens</th>\n",
       "      <th>n_domain_ats</th>\n",
       "      <th>n_domain_masques</th>\n",
       "      <th>domain_entropy</th>\n",
       "      <th>is_top500_domain</th>\n",
       "      <th>n_netloc_dots</th>\n",
       "      <th>len_netloc</th>\n",
       "      <th>n_netloc_num</th>\n",
       "      <th>n_netloc_let</th>\n",
       "      <th>n_netloc_spec</th>\n",
       "      <th>pc_netloc_num</th>\n",
       "      <th>pc_netloc_let</th>\n",
       "      <th>pc_netloc_spec</th>\n",
       "      <th>n_netloc_tok</th>\n",
       "      <th>n_subdomains</th>\n",
       "      <th>avg_netloc_tok_len</th>\n",
       "      <th>n_netloc_masques</th>\n",
       "      <th>netloc_entropy</th>\n",
       "      <th>path_items</th>\n",
       "      <th>len_all_paths</th>\n",
       "      <th>n_path_slashes</th>\n",
       "      <th>n_path_pc20</th>\n",
       "      <th>pc_path_num</th>\n",
       "      <th>pc_path_let</th>\n",
       "      <th>pc_path_spec</th>\n",
       "      <th>n_path_masques</th>\n",
       "      <th>path_entropy</th>\n",
       "      <th>shortest_path_len</th>\n",
       "      <th>longest_path_len</th>\n",
       "      <th>n_single_char_path</th>\n",
       "      <th>n_path_items</th>\n",
       "      <th>avg_path_token_len</th>\n",
       "      <th>pc_path_uppercase</th>\n",
       "      <th>pc_path_lowercase</th>\n",
       "      <th>len_param</th>\n",
       "      <th>n_param_num</th>\n",
       "      <th>n_param_let</th>\n",
       "      <th>n_param_spec</th>\n",
       "      <th>pc_param_num</th>\n",
       "      <th>pc_param_let</th>\n",
       "      <th>pc_param_spec</th>\n",
       "      <th>n_params_masque</th>\n",
       "      <th>param_entropy</th>\n",
       "      <th>n_queries</th>\n",
       "      <th>len_query</th>\n",
       "      <th>n_query_num</th>\n",
       "      <th>n_query_let</th>\n",
       "      <th>n_query_spec</th>\n",
       "      <th>pc_query_num</th>\n",
       "      <th>pc_query_let</th>\n",
       "      <th>pc_query_spec</th>\n",
       "      <th>n_queries_masques</th>\n",
       "      <th>queries_entropy</th>\n",
       "      <th>len_frag</th>\n",
       "      <th>n_frag_num</th>\n",
       "      <th>n_frag_let</th>\n",
       "      <th>n_fraf_spec</th>\n",
       "      <th>pc_frag_num</th>\n",
       "      <th>pc_frag_let</th>\n",
       "      <th>pc_frag_spec</th>\n",
       "      <th>n_frag_masques</th>\n",
       "      <th>frag_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>http://gotomeeting.com/en-fi</td>\n",
       "      <td>benign</td>\n",
       "      <td>http</td>\n",
       "      <td>gotomeeting.com</td>\n",
       "      <td>/en-fi</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>[http, ://, gotomeeting, ., com, /, en, -, fi]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.847883</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>gotomeeting.com</td>\n",
       "      <td>com</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>-14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-0.933333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429423</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429423</td>\n",
       "      <td>[en-fi]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>http://hsqz.china.com.cn</td>\n",
       "      <td>benign</td>\n",
       "      <td>http</td>\n",
       "      <td>hsqz.china.com.cn</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>[http, ://, hsqz, ., china, ., com, ., cn]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.469361</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>china.com.cn</td>\n",
       "      <td>com.cn</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>-13</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-1.083333</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.028321</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1.207340</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>http://tabnak.ir/fa/news/1006112/اولین-کارت-زر...</td>\n",
       "      <td>benign</td>\n",
       "      <td>http</td>\n",
       "      <td>tabnak.ir</td>\n",
       "      <td>/fa/news/1006112/اولین-کارت-زرد-مجلس-یازدهم-به...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>68</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>[http, ://, tabnak, ., ir, /, fa, /, news, /, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.227825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>tabnak.ir</td>\n",
       "      <td>ir</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>-14</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-1.555556</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352214</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352214</td>\n",
       "      <td>[fa, news, 1006112, اولین-کارت-زرد-مجلس-یازدهم...</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>2.123962</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>http://cqhot.cn/category.htmlcat=2107</td>\n",
       "      <td>benign</td>\n",
       "      <td>http</td>\n",
       "      <td>cqhot.cn</td>\n",
       "      <td>/category.htmlcat=2107</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>[http, ://, cqhot, ., cn, /, category, ., html...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.561548</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>cqhot.cn</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>-14</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>[category.htmlcat=2107]</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719897</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>http://td.com/ca/en/business-banking/small-bus...</td>\n",
       "      <td>benign</td>\n",
       "      <td>http</td>\n",
       "      <td>td.com</td>\n",
       "      <td>/ca/en/business-banking/small-business/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>[http, ://, td, ., com, /, ca, /, en, /, busin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.455655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>td.com</td>\n",
       "      <td>com</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>-14</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-2.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[ca, en, business-banking, small-business]</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0</td>\n",
       "      <td>2.375007</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url category scheme             netloc                                               path params query fragment  len_url  is_53  is_54_75  is_76  len_tokenized_url  avg_token_len                                      tokenized_url  last_slashes  loc_last_slashes  n_let  n_num  n_spec    pc_num    pc_let   pc_spec  n_dots  n_ats  n_semicol  num_underscores  num_question  pc_uppercase  pc_lowercase   entropy  n_masques  char_cont_rate       reg_domain domain_suffix  n_domain_suffix  len_domain  is_ip  n_domain_num  n_domain_let  n_domain_spec  pc_domain_num  pc_domain_let  pc_domain_spec  n_domain_dots  n_domain_tok  avg_domain_tok_len  n_domain_hyphens  n_domain_ats  n_domain_masques  domain_entropy  is_top500_domain  n_netloc_dots  len_netloc  n_netloc_num  n_netloc_let  n_netloc_spec  pc_netloc_num  pc_netloc_let  pc_netloc_spec  n_netloc_tok  n_subdomains  avg_netloc_tok_len  n_netloc_masques  netloc_entropy  \\\n",
       "6229                        http://gotomeeting.com/en-fi   benign   http    gotomeeting.com                                             /en-fi                             28   True     False  False                  9       3.111111     [http, ://, gotomeeting, ., com, /, en, -, fi]             5          0.178571     22      0       6  0.000000  0.785714  0.214286       1      0          0                0             0           0.0           1.0  1.847883          0        0.500000  gotomeeting.com           com                1          15  False            15            14            -14       1.000000       0.933333       -0.933333              1             2                 7.5                 0             0                 0        1.429423             False              1          15             0            14              1            0.0       0.933333        0.066667             2             0                7.50                 0        1.429423   \n",
       "6806                            http://hsqz.china.com.cn   benign   http  hsqz.china.com.cn                                                                                24   True     False  False                  9       2.666667         [http, ://, hsqz, ., china, ., com, ., cn]             5          0.208333     18      0       6  0.000000  0.750000  0.250000       3      0          0                0             0           0.0           1.0  1.469361          0        0.333333     china.com.cn        com.cn                2          12  False            15            10            -13       1.250000       0.833333       -1.083333              2             3                 4.0                 0             0                 0        1.028321              True              3          17             0            14              3            0.0       0.823529        0.176471             4             1                4.25                 0        1.207340   \n",
       "10568  http://tabnak.ir/fa/news/1006112/اولین-کارت-زر...   benign   http          tabnak.ir  /fa/news/1006112/اولین-کارت-زرد-مجلس-یازدهم-به...                             68  False      True  False                 25       2.720000  [http, ://, tabnak, ., ir, /, fa, /, news, /, ...             5          0.073529     47      7      14  0.102941  0.691176  0.205882       1      0          0                0             0           0.0           1.0  2.227825          0        0.720588        tabnak.ir            ir                1           9  False            15             8            -14       1.666667       0.888889       -1.555556              1             2                 4.5                 0             0                 0        0.352214             False              1           9             0             8              1            0.0       0.888889        0.111111             2             0                4.50                 0        0.352214   \n",
       "2596               http://cqhot.cn/category.htmlcat=2107   benign   http           cqhot.cn                             /category.htmlcat=2107                             37   True     False  False                 11       3.363636  [http, ://, cqhot, ., cn, /, category, ., html...             5          0.135135     26      4       7  0.108108  0.702703  0.189189       2      0          0                0             0           0.0           1.0  1.561548          0        0.405405         cqhot.cn            cn                1           8  False            15             7            -14       1.875000       0.875000       -1.750000              1             2                 4.0                 0             0                 0        0.375000             False              1           8             0             7              1            0.0       0.875000        0.125000             2             0                4.00                 0        0.375000   \n",
       "10919  http://td.com/ca/en/business-banking/small-bus...   benign   http             td.com            /ca/en/business-banking/small-business/                             52   True     False  False                 18       2.888889  [http, ://, td, ., com, /, ca, /, en, /, busin...             5          0.096154     41      0      11  0.000000  0.788462  0.211538       1      0          0                0             0           0.0           1.0  2.455655          0        0.211538           td.com           com                1           6  False            15             5            -14       2.500000       0.833333       -2.333333              1             2                 3.0                 0             0                 0        0.000000              True              1           6             0             5              1            0.0       0.833333        0.166667             2             0                3.00                 0        0.000000   \n",
       "\n",
       "                                              path_items  len_all_paths  n_path_slashes  n_path_pc20  pc_path_num  pc_path_let  pc_path_spec  n_path_masques  path_entropy  shortest_path_len  longest_path_len  n_single_char_path  n_path_items  avg_path_token_len  pc_path_uppercase  pc_path_lowercase  len_param  n_param_num  n_param_let  n_param_spec  pc_param_num  pc_param_let  pc_param_spec  n_params_masque  param_entropy  n_queries  len_query  n_query_num  n_query_let  n_query_spec  pc_query_num  pc_query_let  pc_query_spec  n_queries_masques  queries_entropy  len_frag  n_frag_num  n_frag_let  n_fraf_spec  pc_frag_num  pc_frag_let  pc_frag_spec  n_frag_masques  frag_entropy  \n",
       "6229                                             [en-fi]              6               1            0     0.000000     0.666667      0.333333               0      0.000000                  5                 5                   0             1                6.00                0.0                1.0          0            0            0             0           0.0           0.0            0.0                0            0.0          0          0            0            0             0           0.0           0.0            0.0                  0              0.0         0           0           0            0          0.0          0.0           0.0               0           0.0  \n",
       "6806                                                  []              0               0            0     0.000000     0.000000      0.000000               0      0.000000                  0                 0                   0             0                 NaN                0.0                0.0          0            0            0             0           0.0           0.0            0.0                0            0.0          0          0            0            0             0           0.0           0.0            0.0                  0              0.0         0           0           0            0          0.0          0.0           0.0               0           0.0  \n",
       "10568  [fa, news, 1006112, اولین-کارت-زرد-مجلس-یازدهم...             52               4            0     0.134615     0.673077      0.192308               0      2.123962                  2                35                   0             4               13.00                0.0                1.0          0            0            0             0           0.0           0.0            0.0                0            0.0          0          0            0            0             0           0.0           0.0            0.0                  0              0.0         0           0           0            0          0.0          0.0           0.0               0           0.0  \n",
       "2596                             [category.htmlcat=2107]             22               1            0     0.181818     0.681818      0.136364               0      0.719897                 21                21                   0             1               22.00                0.0                1.0          0            0            0             0           0.0           0.0            0.0                0            0.0          0          0            0            0             0           0.0           0.0            0.0                  0              0.0         0           0           0            0          0.0          0.0           0.0               0           0.0  \n",
       "10919         [ca, en, business-banking, small-business]             39               5            0     0.000000     0.820513      0.179487               0      2.375007                  2                16                   0             4                9.75                0.0                1.0          0            0            0             0           0.0           0.0            0.0                0            0.0          0          0            0            0             0           0.0           0.0            0.0                  0              0.0         0           0           0            0          0.0          0.0           0.0               0           0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop categorical features\n",
    "drop = ['url', 'scheme', 'netloc', 'path', 'params', 'query', 'fragment', 'reg_domain', 'domain_suffix', 'path_items', 'tokenized_url'\n",
    "]\n",
    "df2 = df.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change booleans to int\n",
    "df2['is_53'] = df2['is_53'].astype(int)\n",
    "df2['is_54_75'] = df2['is_54_75'].astype(int)\n",
    "df2['is_76'] = df2['is_76'].astype(int)\n",
    "df2['is_ip'] = df2['is_ip'].astype(int)\n",
    "df2['is_top500_domain'] = df2['is_top500_domain'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of dataframe\n",
    "df2.to_pickle('capstone2_balanced_final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
