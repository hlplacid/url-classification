{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping to collect list of benign urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used to collect 25,000+ urls by crawling a list of Alexa top websites. The urls were subsequently submitted to virustotal.com, to verify they have a clean reputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in list of domains to crawl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "top_2500 = pd.read_csv('top_2500.csv')\n",
    "top_2500  = top_2500.domain.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get a list of proxies to use\n",
    "\n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "proxies = get_proxies()\n",
    "\n",
    "proxies  # to use with get_all_website_links function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import colorama\n",
    "import random\n",
    "\n",
    "# init the colorama module\n",
    "colorama.init()\n",
    "\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    \n",
    "    proxies =  ['103.102.14.128:8080',\n",
    " '118.172.51.110:36552',\n",
    " '118.174.220.14:43473',\n",
    " '159.65.135.75:8080',\n",
    " '163.172.226.142:3838',\n",
    " '177.99.206.82:8080',\n",
    " '182.160.117.130:53281',\n",
    " '200.108.183.2:8080',\n",
    " '212.233.109.70:3128',\n",
    " '86.125.112.230:57373'] #######  copy in list of proxies\n",
    "    \n",
    "    #proxy_pool = cycle(proxies) \n",
    "    proxy = random.choice(proxies)\n",
    "    \n",
    "    try: \n",
    "        soup = BeautifulSoup(requests.get(url, proxies={\"http\": proxy, \"https\": proxy}).content, \"html.parser\") ####\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            href = a_tag.attrs.get(\"href\")\n",
    "            if href == \"\" or href is None:\n",
    "                # href empty tag\n",
    "                continue\n",
    "            # join the URL if it's relative (not absolute link)\n",
    "            href = urljoin(url, href)\n",
    "            parsed_href = urlparse(href)\n",
    "            # remove URL GET parameters, URL fragments, etc.\n",
    "            #href = parsed_href \n",
    "            # will request all url segments below: \n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path + parsed_href.params + parsed_href.query + parsed_href.fragment\n",
    "            if not is_valid(href):\n",
    "                # not a valid URL\n",
    "                continue\n",
    "            if href in internal_urls:\n",
    "                # already in the set\n",
    "                continue\n",
    "            if domain_name not in href:\n",
    "                # external link\n",
    "                if href not in external_urls:\n",
    "                    print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                    external_urls.add(href)\n",
    "                continue\n",
    "            print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "            urls.add(href)\n",
    "            internal_urls.add(href)\n",
    "        return urls\n",
    "    except:\n",
    "        #Most free proxies will often get connection errors.\n",
    "        #We will skip retries\n",
    "        print(\"Skipping. Connnection error\")\n",
    "\n",
    "def crawl(url, max_urls=30):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 30.\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    links = get_all_website_links(url)\n",
    "    print(links)\n",
    "    if links != None:    \n",
    "        for link in links:\n",
    "            if total_urls_visited > max_urls:\n",
    "                break\n",
    "            crawl(link, max_urls=max_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the domain list and crawl sites\n",
    "\n",
    "for site in top_2500:\n",
    "    crawl('http://' + site,  max_urls = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both internal and external urls (unique list)\n",
    "alexa_urls = internal_urls.union(external_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alexa_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe and create new features by parsing url components\n",
    "df = pd.DataFrame(data = alexa_urls, columns = ['url'])\n",
    "df['scheme'],df['netloc'],df['path'],df['params'],df['query'],df['fragment'] = zip(*df['url'].map(urlparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv file\n",
    "df.to_csv('alexa_urls.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
