{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project's approach will rely mainly on lexical features derived from individual url links. Therefore, afer import of collected benign, phishing and malicious url lists, new features will be created from the base url links. \n",
    "\n",
    "Besides lexical features, this project will leverage www.alexa.com to determine if a url's domain exists in Alexa's Top 500 website list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "#import relevant modules\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sources:\n",
    "\n",
    "Malicious URLs:\n",
    "A sample of 10,000 urls is taken from a csv record of over 600,000 malicious url links retrieved from https://urlhaus.abuse.ch. URLhaus is a project operated by abuse.ch. The project collects and shares malware URLs, to assist network administrators and security analysts in protecting their networks from cyber threats.\n",
    "\n",
    "Phishing URLs:\n",
    "A sample of 10,000 urls is taken from a csv record of over 17,000 phishing url links retrieved from http://phishtank.org/. PhishTank is a collaborative clearing house for data and information about phishing on the Web. It's url lists are available to developers to integrate anti-phishing data into their applications.  \n",
    "\n",
    "Benign URLs:\n",
    "Over 25,000 urls were collected by crawling Alexa's list of the top 2500 websites. In order to help validate that each url was 'benign', each url's reputation was checked via VirusTotal. VirusTotal inspects urls with over 70 antivirus scanners and URL/domain blacklisting services, as well as other tools. Virus scans were requested in those instances where a url had no previous scans or reporting available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv file of phishing urls\n",
    "ph_df = pd.read_csv('phishing_urls.csv')     \n",
    "\n",
    "# Reduce dataset to reflect only urls verified and online\n",
    "ph_df = ph_df[(ph_df['verified'] == 'yes') & (ph_df['online'] == 'yes')]\n",
    "\n",
    "# drop unnecessary features\n",
    "drop = ['phish_id', 'target', 'phish_detail_url', 'submission_time', 'verification_time', 'verified', 'online']\n",
    "ph_df = ph_df.drop(drop, axis=1)\n",
    "\n",
    "# assign category value 'phishing'\n",
    "ph_df['category'] = 'phishing'\n",
    "\n",
    "# take a sample of 10,000 records\n",
    "ph_df_sample = ph_df.sample(n=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in benign urls, drop unnecessary features\n",
    "b_df = pd.read_csv('alexa_urls.csv')  \n",
    "drop = ['scheme', 'netloc', 'path', 'params', 'query', 'fragment']\n",
    "b_df = b_df.drop(drop, axis=1)\n",
    "\n",
    "# assign category value 'benign'\n",
    "b_df['category'] = 'benign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in malicious urls\n",
    "m_df = pd.read_csv('malicious_urls.csv')\n",
    "\n",
    "# assign category value 'malicious'\n",
    "m_df['category'] = 'malicious'\n",
    "\n",
    "# take a sample of 10,000 records\n",
    "m_df_sample = m_df.sample(n=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat all dataframes\n",
    "df = pd.concat([b_df, ph_df_sample, m_df_sample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy\n",
    "df.to_pickle('capstone2_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build out new features based on lexical analysis of the url and its components: scheme, netloc, path, params, query and fragment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('capstone2_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the urls into components: scheme, netloc, path, params, query and fragment.\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "df['scheme'],df['netloc'],df['path'],df['params'],df['query'],df['fragment'] = zip(*df['url'].map(urlparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build url length features\n",
    "df['len_url'] = df['url'].apply(len) # url length\n",
    "df['is_53'] = (df['len_url'] < 54) # is url less than 54 char\n",
    "df['is_54_75'] = (df['len_url'] > 53) & (df['len_url'] < 76) # is url between 54 and 75 char\n",
    "df['is_76'] = (df['len_url'] > 75) # is url greater than 75 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split url into tokens and create new features: list of url's tokens, number of url tokens, average token length \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenized_url = []\n",
    "len_tokenized = []\n",
    "\n",
    "for url in df.url:\n",
    "    result = WordPunctTokenizer().tokenize(url)\n",
    "    tokenized_url.append(result)\n",
    "    len_tokenized.append(len(result))\n",
    "    \n",
    "df['len_tokenized_url'] = len_tokenized\n",
    "df['avg_token_len'] = df['len_url']/df['len_tokenized_url'] \n",
    "df['tokenized_url'] = tokenized_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new numeric feature identifying location of last set of '//' in url\n",
    "df['last_slashes'] = df.apply(lambda row: row.url.rfind('//'), axis=1)\n",
    "\n",
    "# create feature speciyfing % location of slashes within the url\n",
    "df['loc_last_slashes'] = df['last_slashes']/df['len_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features reflecting character content\n",
    "\n",
    "def pc_upper_lower(string):\n",
    "    # Based on the a-zA-Z characters in a string, \n",
    "    # calculates the percentage of uppercase and lowercase characters.\n",
    "    \n",
    "    alpha = uppercase = lowercase = n_letters = 0 \n",
    "    if len(string) >= 1:\n",
    "        for char in string:\n",
    "            if char.isalpha():\n",
    "                alpha += 1\n",
    "                if char.isupper():\n",
    "                    uppercase += 1\n",
    "                if char.islower():\n",
    "                    lowercase += 1\n",
    "        n_letters = uppercase + lowercase\n",
    "        if n_letters != 0:\n",
    "            percent_upper = round(uppercase/n_letters, 3)\n",
    "            percent_lower = round(lowercase/n_letters, 3)\n",
    "        else:\n",
    "            percent_upper = 0\n",
    "            percent_lower = 0\n",
    "    else:\n",
    "        percent_upper = 0\n",
    "        percent_lower = 0 \n",
    "    return percent_upper, percent_lower\n",
    "\n",
    "\n",
    "url_list = df['url'].tolist()\n",
    "\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "\n",
    "percent_alpha = []\n",
    "percent_num = []\n",
    "percent_special = []\n",
    "\n",
    "num_dots = []\n",
    "num_at_signs = []\n",
    "num_semicolons = []\n",
    "num_underscores = []\n",
    "num_question_marks = []\n",
    "upper_percent = []\n",
    "lower_percent = []\n",
    "\n",
    "for i in url_list:\n",
    "    numbers = sum(c.isdigit() for c in i)\n",
    "    num.append(numbers)\n",
    "    percent_num.append(numbers/len(i))\n",
    "    \n",
    "    letters = sum(c.isalpha() for c in i)\n",
    "    let.append(letters)\n",
    "    percent_alpha.append(letters/len(i))\n",
    "    \n",
    "    others = len(i) - numbers - letters\n",
    "    spec.append(others)\n",
    "    percent_special.append(others/len(i))\n",
    "    \n",
    "    num_dots.append(i.count('.'))\n",
    "    num_at_signs.append(i.count('@'))\n",
    "    num_semicolons.append(i.count(';'))\n",
    "    num_underscores.append(i.count('_'))\n",
    "    num_question_marks.append(i.count('?'))\n",
    "\n",
    "    percent_upper, percent_lower = pc_upper_lower(i)\n",
    "    upper_percent.append(percent_upper)\n",
    "    lower_percent.append(percent_lower)\n",
    "\n",
    "df['n_let'] = let \n",
    "df['n_num'] = num\n",
    "df['n_spec'] = spec\n",
    "df['pc_num'] = percent_num\n",
    "df['pc_let'] = percent_alpha\n",
    "df['pc_spec'] = percent_special\n",
    "df['n_dots'] = num_dots\n",
    "df['n_ats'] = num_at_signs\n",
    "df['n_semicol'] = num_semicolons\n",
    "df['num_underscores'] = num_underscores\n",
    "df['num_question'] = num_question_marks\n",
    "df['pc_uppercase'] = upper_percent\n",
    "df['pc_lowercase'] = lower_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate an Shannon entropy score for each url\n",
    "# urls with a larger characters distribution will have a higher score\n",
    "\n",
    "import math\n",
    "\n",
    "def shannon(word):\n",
    "    entropy = 0.0\n",
    "    length = len(word)\n",
    "    occ = {}\n",
    "    for c in word :\n",
    "        if not c in occ:\n",
    "            occ[ c ] = 0\n",
    "        else:\n",
    "            occ[ c ] = occ[c] + 1\n",
    "\n",
    "    for (k,v) in occ.items(): # changed from iteritems\n",
    "        p = float( v ) / float(length)\n",
    "        if p > 0: # added this to avoid math domain error where p = 0\n",
    "            entropy -= p * math.log(p, 2) # Log base 2\n",
    "    return entropy\n",
    "\n",
    "url_entropy_result = []\n",
    "\n",
    "for i in url_list:\n",
    "    url_entropy_result.append(shannon(i))\n",
    "    \n",
    "df['entropy'] = url_entropy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a masque feature: the number of letter+digit+letter substrings in a url\n",
    "# create a character continuity rate feature: (length of longest letter substring + length of longest digit substring +\n",
    "# length of longest special character substring) / URL length\n",
    "\n",
    "import re\n",
    "\n",
    "def masque_count(token):\n",
    "    match = re.findall(\"([a-zA-Z][0-9][a-zA-Z])\", token)\n",
    "    match_count = len(match)\n",
    "    return match_count\n",
    "\n",
    "def longestSubstring(str):\n",
    "    \n",
    "    # find the longest consecutive substring of a certain type\n",
    "    \n",
    "    l = re.findall(r'[A-Za-z]+', str)\n",
    "    d = re.findall(r'\\d+', str)\n",
    "    s = re.findall(r'[^a-zA-Z0-9]+', str)\n",
    "    if l:  #THESE WONT CALC IF EMPTY LIST\n",
    "        ll = max(l, key = len)\n",
    "        max_l = len(ll)\n",
    "    else:\n",
    "        max_l = 0\n",
    "    if d: \n",
    "        ld = max(d, key = len)\n",
    "        max_d = len(ld)\n",
    "    else:\n",
    "        max_d = 0\n",
    "    if s:\n",
    "        ls = max(s, key = len)\n",
    "        max_s = len(ls)\n",
    "    else:\n",
    "        max_s = 0\n",
    "        \n",
    "    char_total = (max_l + max_d + max_s)\n",
    "    char_cont_rate = char_total/len(str)\n",
    "    return char_cont_rate\n",
    "\n",
    "\n",
    "url_masques = [] \n",
    "cont_rate = []\n",
    "                    \n",
    "for url in url_list:\n",
    "    url_masques.append(masque_count(url))\n",
    "    cont_rate.append(longestSubstring(url))\n",
    "\n",
    "df['n_masques'] = url_masques\n",
    "df['char_cont_rate'] = cont_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reg_domain and domain_suffix features, by extracting second-level and top-level domain section. e.g. example.com\n",
    "# identify ip addresses used in place of domain\n",
    "\n",
    "# import tldextract to parse out true registered domain\n",
    "import tldextract \n",
    "\n",
    "# adding a function that identifies an ip address\n",
    "def is_ipv4(ip):\n",
    "    match = re.match(\"^(\\d{0,3})\\.(\\d{0,3})\\.(\\d{0,3})\\.(\\d{0,3})\", ip)\n",
    "    if not match:\n",
    "        return False\n",
    "    quad = []\n",
    "    for number in match.groups():\n",
    "        quad.append(int(number))\n",
    "    if quad[0] < 1:\n",
    "        return False\n",
    "    for number in quad:\n",
    "        if number > 255 or number < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "reg_domain = []\n",
    "domain_suffix = []\n",
    "\n",
    "for i in url_list:\n",
    "    ext = tldextract.extract(i)\n",
    "    reg = '.'.join(ext[1:]) #this returns domain + suffixes or ip + .\n",
    "    sub_domain = '.'.join(ext[:2])\n",
    "    suffix = ext.suffix\n",
    "    domain_suffix.append(suffix)\n",
    "    if is_ipv4(reg): # if reg is an ip, drop the . at the end of ip\n",
    "        reg = str(reg)[:-1]\n",
    "    \n",
    "    reg_domain.append(reg) #append list with the domain+suffix, or ip address\n",
    "    \n",
    "#add domain as new df feature \n",
    "df['reg_domain'] = reg_domain\n",
    "df['domain_suffix'] = domain_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature identifying the number of domain suffixes (top-level domains)\n",
    "\n",
    "num_domain_suffix = []\n",
    "\n",
    "for i in df.domain_suffix:\n",
    "    if i:\n",
    "        num_domain_suffix.append(i.count('.') + 1)\n",
    "    else: \n",
    "        num_domain_suffix.append(0)\n",
    "\n",
    "\n",
    "df['n_domain_suffix'] = num_domain_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use prior is_ip function to create new boolean feature. \n",
    "# create new features based on reg_domain characters\n",
    "\n",
    "IP = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = []\n",
    "domain_dots = []\n",
    "domain_len = []\n",
    "domain_hyphens = []\n",
    "domain_ats = []\n",
    "domain_masques = []\n",
    "domain_entropy_result = []\n",
    "\n",
    "for i in reg_domain:\n",
    "    response = is_ipv4(i)\n",
    "    IP.append(response)\n",
    "    \n",
    "    num.append(numbers)\n",
    "    percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "    letters = sum(c.isalpha() for c in i)\n",
    "    let.append(letters)\n",
    "    percent_chars.append(letters/len(i))\n",
    "    \n",
    "    others = len(i) - numbers - letters\n",
    "    spec.append(others)\n",
    "    percent_others.append(others/len(i))\n",
    "    \n",
    "    domain_dots.append(i.count('.'))\n",
    "    domain_len.append(len(i))\n",
    "    domain_hyphens.append(i.count('-'))\n",
    "    domain_ats.append(i.count('@'))                  \n",
    "                    \n",
    "    domain_masques.append(masque_count(i))\n",
    "    domain_entropy_result.append(shannon(i))\n",
    "\n",
    "df['len_domain'] = domain_len\n",
    "df['is_ip'] = IP #add boolean list to df\n",
    "df['n_domain_num'] = num\n",
    "df['n_domain_let'] = let\n",
    "df['n_domain_spec'] = spec\n",
    "df['pc_domain_num'] = percent_numbers\n",
    "df['pc_domain_let'] = percent_chars\n",
    "df['pc_domain_spec'] = percent_others\n",
    "df['n_domain_dots'] = domain_dots\n",
    "df['n_domain_tok'] = df['n_domain_dots'] + 1\n",
    "df['avg_domain_tok_len'] = df['len_domain']/df['n_domain_tok']\n",
    "df['n_domain_hyphens'] = domain_hyphens\n",
    "df['n_domain_ats'] = domain_ats\n",
    "df['n_domain_masques'] = domain_masques\n",
    "df['domain_entropy'] = domain_entropy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create is_top_500_domain (a boolean feature) to report if reg_domain is in Alexa's top 500 domain list\n",
    "\n",
    "# read in top 500 websites\n",
    "top_500_domains = pd.read_csv('alexa_top500.csv')\n",
    "\n",
    "top_500_list = top_500_domains.domain.to_list()\n",
    "\n",
    "#adding a function that identifies whether a domain is in the top 500 domain list\n",
    "def is_match(domain, my_list):\n",
    "    if domain in my_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_top_500_domain = []\n",
    "\n",
    "for domain in reg_domain:\n",
    "    if is_ipv4(domain):\n",
    "        is_top_500_domain.append(False)\n",
    "    else:\n",
    "        is_top_500_domain.append(is_match(domain, top_500_list))\n",
    "    \n",
    "df['is_top500_domain'] = is_top_500_domain # likely FALSE for suspicious/malicious urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features based on url's netloc section (netloc = subdomain + domain + suffix) in comparison to exisitng reg_domain feature\n",
    "\n",
    "num_netloc_dots = []\n",
    "len_netloc = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "netloc_masques = []\n",
    "netloc_entropy_result = []\n",
    "n_subs = []\n",
    "\n",
    "netloc = df.netloc\n",
    "\n",
    "for i in netloc:\n",
    "    dots = i.count('.')\n",
    "    num_netloc_dots.append(dots)\n",
    "    len_netloc.append(len(i))\n",
    "\n",
    "    numbers = sum(c.isdigit() for c in i)\n",
    "    num.append(numbers)\n",
    "    percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "    letters = sum(c.isalpha() for c in i)\n",
    "    let.append(letters)\n",
    "    percent_chars.append(letters/len(i))\n",
    "    \n",
    "    others = len(i) - numbers - letters\n",
    "    spec.append(others)\n",
    "    percent_others.append(others/len(i))\n",
    "    \n",
    "    netloc_masques.append(masque_count(i))\n",
    "    \n",
    "    netloc_entropy_result.append(shannon(i))\n",
    "    \n",
    "\n",
    "df['n_netloc_dots'] = num_netloc_dots\n",
    "df['len_netloc'] = len_netloc\n",
    "df['n_netloc_num'] = num\n",
    "df['n_netloc_let'] = let\n",
    "df['n_netloc_spec'] = spec\n",
    "df['pc_netloc_num'] = percent_numbers\n",
    "df['pc_netloc_let'] = percent_chars\n",
    "df['pc_netloc_spec'] = percent_others\n",
    "df['n_netloc_tok'] = df['n_netloc_dots'] + 1\n",
    "df['n_subdomains'] = (df['n_netloc_tok'] - df['n_domain_tok'])\n",
    "df['avg_netloc_tok_len'] = df['len_netloc']/(df['n_netloc_tok'])\n",
    "df['n_netloc_masques'] = netloc_masques\n",
    "df['netloc_entropy'] = netloc_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature with list of all paths within url\n",
    "\n",
    "path_items = []\n",
    "\n",
    "for i in df.path:\n",
    "    path_list = (re.split('/', i))\n",
    "    path_list = [x for x in path_list if x != \"\"]\n",
    "    path_items.append(path_list)\n",
    "    \n",
    "df['path_items'] = path_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional path features based on characters\n",
    "\n",
    "path_slashes = []\n",
    "path_20 = []\n",
    "len_total_path = []\n",
    "percent_numbers = []\n",
    "percent_letters = []\n",
    "percent_others = [] \n",
    "path_masques = []\n",
    "path_entropy_result = []\n",
    "\n",
    "for i in df.path:\n",
    "    path_slashes.append(i.count('/'))\n",
    "    path_20.append(i.count('/%20'))\n",
    "    len_total_path.append(len(i))\n",
    "    \n",
    "    if len(i) > 0:\n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        percent_letters.append(letters/len(i))\n",
    "        others = len(i) - numbers - letters\n",
    "        percent_others.append(others/len(i))\n",
    "        path_masques.append(masque_count(i))\n",
    "        path_entropy_result.append(shannon(i))\n",
    "    \n",
    "    else:\n",
    "        percent_numbers.append(0)\n",
    "        percent_letters.append(0)\n",
    "        percent_others.append(0)\n",
    "        path_masques.append(0)\n",
    "        path_entropy_result.append(0)\n",
    "\n",
    "df['len_all_paths'] = len_total_path    \n",
    "df['n_path_slashes'] = path_slashes\n",
    "df['n_path_pc20'] = path_20\n",
    "df['pc_path_num'] = percent_numbers\n",
    "df['pc_path_let'] = percent_letters\n",
    "df['pc_path_spec'] = percent_others\n",
    "df['n_path_masques'] = path_masques\n",
    "df['path_entropy'] = path_entropy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path features based on length of individual path items\n",
    "\n",
    "def path_lengths(list_of_paths):\n",
    "    max = 0\n",
    "    min = 500\n",
    "    single = 0\n",
    "    for i in list_of_paths:\n",
    "        if len(i) < min:\n",
    "             min = len(i)    \n",
    "        if len(i) > max:\n",
    "            max = len(i)    \n",
    "        if len(i) == 1: \n",
    "            single += 1        \n",
    "    num_items = len(list_of_paths)    \n",
    "    return min, max, single, num_items\n",
    "\n",
    "path_shortest_item_len = []\n",
    "path_longest_item_len = []\n",
    "num_single_char_path = []\n",
    "num_path_items = []\n",
    "\n",
    "for paths in df.path_items:\n",
    "    if len(paths) > 0:\n",
    "        min, max, single, num_items = path_lengths(paths)\n",
    "        path_shortest_item_len.append(min)\n",
    "        path_longest_item_len.append(max)\n",
    "        num_single_char_path.append(single)\n",
    "        num_path_items.append(num_items)\n",
    "    else:\n",
    "        path_shortest_item_len.append(0)\n",
    "        path_longest_item_len.append(0)\n",
    "        num_single_char_path.append(0)\n",
    "        num_path_items.append(0)\n",
    "    \n",
    "df['shortest_path_len'] = path_shortest_item_len\n",
    "df['longest_path_len'] = path_longest_item_len\n",
    "df['n_single_char_path'] = num_single_char_path\n",
    "df['n_path_items'] = num_path_items\n",
    "df['avg_path_token_len'] = (df['len_all_paths'])/(df['n_path_items'])\n",
    "\n",
    "# switch out any 'inf' values with NAN\n",
    "df.loc[~np.isfinite(df['avg_path_token_len']), 'avg_path_token_len'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Path section, calculate percent of upper and lowercase letters (of total letters)\n",
    "\n",
    "upper_percent = []\n",
    "lower_percent = []\n",
    "\n",
    "paths = df.path\n",
    "\n",
    "for i in paths:\n",
    "    percent_upper, percent_lower = pc_upper_lower(i)\n",
    "    upper_percent.append(percent_upper)\n",
    "    lower_percent.append(percent_lower)\n",
    "    \n",
    "df['pc_path_uppercase'] = upper_percent\n",
    "df['pc_path_lowercase'] = lower_percent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features based on parameter section of url\n",
    "\n",
    "len_parameters = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "params_masques = []\n",
    "param_entropy_result = []\n",
    "\n",
    "for i in df.params:\n",
    "    length = len(i)\n",
    "    len_parameters.append(length)\n",
    "    \n",
    "    if i:\n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        num.append(numbers)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        let.append(letters)\n",
    "        percent_chars.append(letters/len(i))\n",
    "    \n",
    "        others = len(i) - numbers - letters\n",
    "        spec.append(others)\n",
    "        percent_others.append(others/len(i))\n",
    "    \n",
    "        params_masques.append(masque_count(i))\n",
    "    \n",
    "        param_entropy_result.append(shannon(i))\n",
    "    \n",
    "    else:\n",
    "        num.append(0)\n",
    "        let.append(0)\n",
    "        spec.append(0)\n",
    "        percent_numbers.append(0)\n",
    "        percent_chars.append(0)\n",
    "        percent_others.append(0)\n",
    "        params_masques.append(0)\n",
    "        param_entropy_result.append(0)\n",
    "        \n",
    "df['len_param'] = len_parameters\n",
    "df['n_param_num'] = num\n",
    "df['n_param_let'] = let\n",
    "df['n_param_spec'] = spec\n",
    "df['pc_param_num'] = percent_numbers\n",
    "df['pc_param_let'] = percent_chars\n",
    "df['pc_param_spec'] = percent_others\n",
    "df['n_params_masque'] = params_masques\n",
    "df['param_entropy'] = param_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features based on query section of url\n",
    "\n",
    "queries = df['query']\n",
    "num_queries = []\n",
    "len_query = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "queries_masques = []\n",
    "queries_entropy_result = []\n",
    "\n",
    "\n",
    "for i in queries:\n",
    "    if len(i) > 0:\n",
    "        num_queries.append(i.count(';') + 1) \n",
    "        len_query.append(len(i))\n",
    "        \n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        num.append(numbers)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "        \n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        let.append(letters)\n",
    "        percent_chars.append(letters/len(i))\n",
    "        \n",
    "        others = len(i) - numbers - letters\n",
    "        spec.append(others)\n",
    "        percent_others.append(others/len(i))\n",
    "    \n",
    "        queries_masques.append(masque_count(i))\n",
    "        \n",
    "        queries_entropy_result.append(shannon(i))\n",
    "    else:\n",
    "        num_queries.append(0)\n",
    "        len_query.append(0)\n",
    "        num.append(0)\n",
    "        let.append(0)\n",
    "        spec.append(0)\n",
    "        percent_numbers.append(0)\n",
    "        percent_chars.append(0)\n",
    "    \n",
    "        percent_others.append(0)\n",
    "    \n",
    "        queries_masques.append(0)\n",
    "    \n",
    "        queries_entropy_result.append(0)\n",
    "        \n",
    "df['n_queries'] = num_queries\n",
    "df['len_query'] = len_query\n",
    "df['n_query_num'] = num\n",
    "df['n_query_let'] = let\n",
    "df['n_query_spec'] = spec\n",
    "df['pc_query_num'] = percent_numbers\n",
    "df['pc_query_let'] = percent_chars\n",
    "df['pc_query_spec'] = percent_others\n",
    "df['n_queries_masques'] = queries_masques\n",
    "df['queries_entropy'] = queries_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fragment Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features based on fragment section\n",
    "\n",
    "len_fragment = []\n",
    "num = []\n",
    "let = []\n",
    "spec = []\n",
    "percent_numbers = []\n",
    "percent_chars = []\n",
    "percent_others = [] \n",
    "frag_masques = []\n",
    "frag_entropy_result = []\n",
    "\n",
    "for i in df.fragment:\n",
    "    len_frag = len(i)\n",
    "    len_fragment.append(len_frag)\n",
    "    \n",
    "    if len_frag > 0:\n",
    "        numbers = sum(c.isdigit() for c in i)\n",
    "        num.append(numbers)\n",
    "        percent_numbers.append(numbers/len(i))\n",
    "    \n",
    "        letters = sum(c.isalpha() for c in i)\n",
    "        let.append(letters)\n",
    "        percent_chars.append(letters/len(i))\n",
    "    \n",
    "        others = len(i) - numbers - letters\n",
    "        spec.append(others)\n",
    "        percent_others.append(others/len(i))\n",
    "    \n",
    "        frag_masques.append(masque_count(i))\n",
    "    \n",
    "        frag_entropy_result.append(shannon(i))\n",
    "    \n",
    "    else:\n",
    "        num.append(0)\n",
    "        let.append(0)\n",
    "        spec.append(0)\n",
    "        percent_numbers.append(0)\n",
    "        percent_chars.append(0)\n",
    "        percent_others.append(0)\n",
    "        frag_masques.append(0)\n",
    "        frag_entropy_result.append(0)\n",
    "        \n",
    "df['len_frag'] = len_fragment\n",
    "df['n_frag_num'] = num\n",
    "df['n_frag_let'] = let\n",
    "df['n_fraf_spec'] = spec\n",
    "df['pc_frag_num'] = percent_numbers\n",
    "df['pc_frag_let'] = percent_chars\n",
    "df['pc_frag_spec'] = percent_others\n",
    "df['n_frag_masques'] = frag_masques\n",
    "df['frag_entropy'] = frag_entropy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of dataframe\n",
    "df.to_pickle('capstone2_withfeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('capstone2_withfeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop categorical features\n",
    "drop = ['url', 'scheme', 'netloc', 'path', 'params', 'query', 'fragment', 'reg_domain', 'domain_suffix', 'path_items', 'tokenized_url'\n",
    "]\n",
    "df2 = df.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change booleans to int\n",
    "df2['is_53'] = df2['is_53'].astype(int)\n",
    "df2['is_54_75'] = df2['is_54_75'].astype(int)\n",
    "df2['is_76'] = df2['is_76'].astype(int)\n",
    "df2['is_ip'] = df2['is_ip'].astype(int)\n",
    "df2['is_top500_domain'] = df2['is_top500_domain'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of dataframe\n",
    "df2.to_pickle('capstone2_final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
